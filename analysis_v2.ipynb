{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef161d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset shape: (782, 69)\n",
      "Columns (first 40): ['Age', 'BMI', 'Sex', 'Height', 'Weight', 'Length_of_Stay', 'Management', 'Severity', 'Diagnosis_Presumptive', 'Diagnosis', 'Alvarado_Score', 'Paedriatic_Appendicitis_Score', 'Appendix_on_US', 'Appendix_Diameter', 'Migratory_Pain', 'Lower_Right_Abd_Pain', 'Contralateral_Rebound_Tenderness', 'Coughing_Pain', 'Nausea', 'Loss_of_Appetite', 'Body_Temperature', 'WBC_Count', 'Neutrophil_Percentage', 'Segmented_Neutrophils', 'Neutrophilia', 'RBC_Count', 'Hemoglobin', 'RDW', 'Thrombocyte_Count', 'Ketones_in_Urine', 'RBC_in_Urine', 'WBC_in_Urine', 'CRP', 'Dysuria', 'Stool', 'Peritonitis', 'Psoas_Sign', 'Ipsilateral_Rebound_Tenderness', 'US_Performed', 'US_Number']\n",
      "Dropping empty columns: ['unnamed:_58', 'unnamed:_59', 'unnamed:_60', 'unnamed:_61', 'unnamed:_62', 'unnamed:_63', 'unnamed:_64', 'unnamed:_65', 'unnamed:_66', 'unnamed:_67', 'unnamed:_68']\n",
      "Duplicate rows: 0\n",
      "Selected target columns: ['management', 'severity', 'diagnosis_presumptive']\n",
      "Random split (no suitable stratify column).\n",
      "Train/test shapes: (625, 55) (157, 55)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 145\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# One-hot encode categorical features (fit on train)\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m X_train_cat.empty:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     ohe = \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mignore\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     X_train_cat_ohe = pd.DataFrame(ohe.fit_transform(X_train_cat), columns=ohe.get_feature_names_out(cat_feats), index=X_train.index)\n\u001b[32m    147\u001b[39m     X_test_cat_ohe = pd.DataFrame(ohe.transform(X_test_cat), columns=ohe.get_feature_names_out(cat_feats), index=X_test.index)\n",
      "\u001b[31mTypeError\u001b[39m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "# Appendicitis dataset pipeline: cleaning, EDA, split-before-impute, modelling, feature importance\n",
    "# Paste into a Jupyter cell and run. Edit `DATA_PATH` if needed.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# --------------------------\n",
    "# 0. Configuration\n",
    "# --------------------------\n",
    "DATA_PATH = \"C:/Users/HP/Desktop/PYTHON PROJECT FROM DATALABS/appendicitis_data.csv\"   # ← change to your csv path if different\n",
    "SAVE_DIR = \"C:/Users/HP/Desktop/PYTHON PROJECT FROM DATALABS\"                          # ← change to a local folder if needed\n",
    "N_FEATURES_SELECT = 20                          # number of top features to keep per target\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --------------------------\n",
    "# 1. Load dataset\n",
    "# --------------------------\n",
    "df = pd.read_csv(\"appendicitis_data.csv\")\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "print(\"Columns (first 40):\", list(df.columns[:40]))\n",
    "\n",
    "# --------------------------\n",
    "# 2. Quick cleaning steps\n",
    "# --------------------------\n",
    "# 2.1 normalize column names\n",
    "df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n",
    "\n",
    "# 2.2 Drop columns that are entirely empty\n",
    "empty_cols = df.columns[df.isna().all()].tolist()\n",
    "if empty_cols:\n",
    "    print(\"Dropping empty columns:\", empty_cols)\n",
    "    df = df.drop(columns=empty_cols)\n",
    "\n",
    "# 2.3 Remove exact duplicate rows\n",
    "dup_count = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {dup_count}\")\n",
    "if dup_count > 0:\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Identify targets (heuristic)\n",
    "# --------------------------\n",
    "# The script tries to find typical column names; change if your target columns differ.\n",
    "candidates = ['diagnosis','diagnose','appendicitis','management','severity','complicated','pas','score','paediatric']\n",
    "targets = []\n",
    "for c in df.columns:\n",
    "    for t in candidates:\n",
    "        if t in c and c not in targets:\n",
    "            targets.append(c)\n",
    "# ensure we have 3 targets (fallback to last 3 columns)\n",
    "if len(targets) < 3:\n",
    "    targets = list(df.columns[-3:])   # fallback\n",
    "targets = targets[:3]\n",
    "print(\"Selected target columns:\", targets)\n",
    "\n",
    "# --------------------------\n",
    "# 4. Basic datatype fixes\n",
    "# --------------------------\n",
    "# Convert object columns that are numeric-like into numeric\n",
    "for c in df.select_dtypes(include=['object']).columns:\n",
    "    cleaned = df[c].astype(str).str.replace('[^0-9.+-]', '', regex=True)\n",
    "    non_empty = cleaned[cleaned!='']\n",
    "    if len(non_empty) > 0:\n",
    "        frac_numeric = non_empty.str.replace('.','',1).str.lstrip('+-').str.isdigit().mean()\n",
    "        if frac_numeric > 0.6:\n",
    "            df[c] = pd.to_numeric(cleaned, errors='coerce')\n",
    "\n",
    "# Replace negative numeric values with NaN for later imputation (common error)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "negative_summary = {c: int((df[c] < 0).sum()) for c in numeric_cols if (df[c] < 0).sum() > 0}\n",
    "if negative_summary:\n",
    "    print(\"Negative values found (set to NaN):\", negative_summary)\n",
    "    for c in negative_summary:\n",
    "        df.loc[df[c] < 0, c] = np.nan\n",
    "\n",
    "# Remove control characters in object fields\n",
    "for c in df.select_dtypes(include=['object']).columns:\n",
    "    df[c] = df[c].astype(str).str.replace(r'[\\x00-\\x1f]', '', regex=True).str.strip()\n",
    "    df.loc[df[c] == '', c] = np.nan\n",
    "\n",
    "# --------------------------\n",
    "# 5. Split into train/test BEFORE imputation\n",
    "# --------------------------\n",
    "features = [c for c in df.columns if c not in targets]\n",
    "X = df[features]\n",
    "y_df = df[targets]\n",
    "\n",
    "# Try to find a good stratify column (one of the targets with at least 2 examples per class)\n",
    "stratify_col = None\n",
    "for t in targets:\n",
    "    vc = y_df[t].value_counts(dropna=True)\n",
    "    if len(vc) > 1 and vc.min() >= 2:\n",
    "        stratify_col = y_df[t]\n",
    "        stratify_target_name = t\n",
    "        break\n",
    "\n",
    "if stratify_col is not None:\n",
    "    print(f\"Stratified split on '{stratify_target_name}'\")\n",
    "    X_train, X_test, y_train_df, y_test_df = train_test_split(X, y_df, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify_col)\n",
    "else:\n",
    "    print(\"Random split (no suitable stratify column).\")\n",
    "    X_train, X_test, y_train_df, y_test_df = train_test_split(X, y_df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Train/test shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 6. Impute missing values (fit imputers on TRAIN only)\n",
    "# --------------------------\n",
    "numeric_feats = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_feats = [c for c in X_train.columns if c not in numeric_feats]\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train[numeric_feats]), columns=numeric_feats, index=X_train.index)\n",
    "X_test_num = pd.DataFrame(num_imputer.transform(X_test[numeric_feats]), columns=numeric_feats, index=X_test.index)\n",
    "\n",
    "# Reduce cardinality on categorical cols (keep top 10 categories in train; others -> '__other__')\n",
    "X_train_cat = pd.DataFrame(index=X_train.index)\n",
    "X_test_cat = pd.DataFrame(index=X_test.index)\n",
    "if cat_feats:\n",
    "    X_train_cat_full = X_train[cat_feats].astype(object).copy()\n",
    "    X_test_cat_full = X_test[cat_feats].astype(object).copy()\n",
    "    for c in cat_feats:\n",
    "        top = X_train_cat_full[c].value_counts().nlargest(10).index.tolist()\n",
    "        X_train_cat_full[c] = X_train_cat_full[c].where(X_train_cat_full[c].isin(top), other='__other__')\n",
    "        X_test_cat_full[c] = X_test_cat_full[c].where(X_test_cat_full[c].isin(top), other='__other__')\n",
    "    X_train_cat = pd.DataFrame(cat_imputer.fit_transform(X_train_cat_full), columns=cat_feats, index=X_train.index)\n",
    "    X_test_cat = pd.DataFrame(cat_imputer.transform(X_test_cat_full), columns=cat_feats, index=X_test.index)\n",
    "\n",
    "# One-hot encode categorical features (fit on train)\n",
    "if not X_train_cat.empty:\n",
    "    # scikit-learn changed the OneHotEncoder parameter name from 'sparse' to 'sparse_output' in newer versions.\n",
    "    # Try the newer parameter first and fall back to the older name for compatibility.\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    X_train_cat_ohe = pd.DataFrame(ohe.fit_transform(X_train_cat), columns=ohe.get_feature_names_out(cat_feats), index=X_train.index)\n",
    "    X_test_cat_ohe = pd.DataFrame(ohe.transform(X_test_cat), columns=ohe.get_feature_names_out(cat_feats), index=X_test.index)\n",
    "else:\n",
    "    X_train_cat_ohe = pd.DataFrame(index=X_train.index)\n",
    "    X_test_cat_ohe = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "# Reconstruct final X_train and X_test (imputed + encoded)\n",
    "X_train_clean = pd.concat([X_train_num, X_train_cat_ohe], axis=1)\n",
    "X_test_clean  = pd.concat([X_test_num, X_test_cat_ohe], axis=1)\n",
    "\n",
    "print(\"After imputation/encoding - shapes:\", X_train_clean.shape, X_test_clean.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 7. Scale: fit scaler on X_train only, apply to both sets\n",
    "# --------------------------\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_clean)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train_clean), columns=X_train_clean.columns, index=X_train_clean.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_clean), columns=X_test_clean.columns, index=X_test_clean.index)\n",
    "\n",
    "# --------------------------\n",
    "# 8. Model training loop (per target)\n",
    "#     - map common textual labels into binary where logical\n",
    "#     - select top K features via mutual_info_classif\n",
    "#     - upsample minority in train\n",
    "#     - train LogisticRegression, RandomForest, KNN, GradientBoost\n",
    "#     - evaluate on test (independent)\n",
    "# --------------------------\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "models = {\n",
    "    'logistic': LogisticRegression(max_iter=1000),\n",
    "    'random_forest': RandomForestClassifier(n_estimators=150, random_state=RANDOM_STATE),\n",
    "    'knn': KNeighborsClassifier(n_neighbors=5),\n",
    "    'gradient_boost': GradientBoostingClassifier(n_estimators=150, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "results = {}  # store each target results\n",
    "\n",
    "for target in targets:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Target:\", target)\n",
    "    # Normalize strings, map to binary for common cases\n",
    "    y_train_raw = y_train_df[target].astype(str).str.lower().str.strip()\n",
    "    y_test_raw = y_test_df[target].astype(str).str.lower().str.strip()\n",
    "\n",
    "    if 'append' in target or 'diagn' in target:\n",
    "        y_train = y_train_raw.apply(lambda s: 0 if ('no' in s or s in ['0','none','nan','na','n']) else 1)\n",
    "        y_test  = y_test_raw.apply(lambda s: 0 if ('no' in s or s in ['0','none','nan','na','n']) else 1)\n",
    "    elif 'manage' in target:\n",
    "        y_train = y_train_raw.apply(lambda s: 1 if ('surg' in s or 'oper' in s or 'lapar' in s or 'appendectomy' in s) else 0)\n",
    "        y_test  = y_test_raw.apply(lambda s: 1 if ('surg' in s or 'oper' in s or 'lapar' in s or 'appendectomy' in s) else 0)\n",
    "    elif 'sever' in target or 'complic' in target:\n",
    "        y_train = y_train_raw.apply(lambda s: 1 if ('complic' in s or 'complicated' in s or 'perfor' in s or 'gangre' in s) else 0)\n",
    "        y_test  = y_test_raw.apply(lambda s: 1 if ('complic' in s or 'complicated' in s or 'perfor' in s or 'gangre' in s) else 0)\n",
    "    else:\n",
    "        # fallback label-encoding of train unique values\n",
    "        uniques = pd.Series(y_train_raw.dropna().unique())\n",
    "        mapping = {v:i for i,v in enumerate(uniques)}\n",
    "        y_train = y_train_raw.map(mapping).fillna(-1).astype(int)\n",
    "        y_test  = y_test_raw.map(mapping).fillna(-1).astype(int)\n",
    "\n",
    "    # keep rows where train mapping exists\n",
    "    valid_train_idx = y_train[y_train != -1].index\n",
    "    valid_test_idx  = y_test[y_test != -1].index\n",
    "\n",
    "    X_tr = X_train_scaled.loc[valid_train_idx]\n",
    "    y_tr = y_train.loc[valid_train_idx]\n",
    "    X_te = X_test_scaled.loc[valid_test_idx]\n",
    "    y_te = y_test.loc[valid_test_idx]\n",
    "\n",
    "    if X_tr.shape[0] == 0 or X_te.shape[0] == 0:\n",
    "        print(\"No valid train/test rows for this target after mapping — skipping.\")\n",
    "        continue\n",
    "\n",
    "    # require at least 2 classes to proceed\n",
    "    if len(y_tr.unique()) < 2:\n",
    "        print(\"Not enough classes in train after mapping — skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Feature selection: mutual_info_classif. Choose top K features\n",
    "    k = min(N_FEATURES_SELECT, X_tr.shape[1])\n",
    "    try:\n",
    "        selector = SelectKBest(mutual_info_classif, k=k)\n",
    "        selector.fit(X_tr.fillna(0), y_tr)\n",
    "        selected_cols = list(X_tr.columns[selector.get_support()])\n",
    "        print(f\"Selected top {len(selected_cols)} features for {target}.\")\n",
    "    except Exception as e:\n",
    "        print(\"Feature selection failed, using all features. Error:\", e)\n",
    "        selected_cols = X_tr.columns.tolist()\n",
    "\n",
    "    X_tr_sel = X_tr[selected_cols]\n",
    "    X_te_sel = X_te[selected_cols]\n",
    "\n",
    "    # Upsample training set to balance classes\n",
    "    df_tr = pd.concat([X_tr_sel, y_tr.rename('target')], axis=1)\n",
    "    max_n = df_tr['target'].value_counts().max()\n",
    "    parts = []\n",
    "    for cl, grp in df_tr.groupby('target'):\n",
    "        if len(grp) < max_n:\n",
    "            parts.append(resample(grp, replace=True, n_samples=max_n, random_state=RANDOM_STATE))\n",
    "        else:\n",
    "            parts.append(grp)\n",
    "    df_bal = pd.concat(parts)\n",
    "    X_tr_bal = df_bal.drop(columns=['target'])\n",
    "    y_tr_bal = df_bal['target']\n",
    "\n",
    "    # Train models and evaluate\n",
    "    target_results = {}\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            model.fit(X_tr_bal, y_tr_bal)\n",
    "            preds = model.predict(X_te_sel)\n",
    "            # Get probabilities where available\n",
    "            prob = None\n",
    "            roc = None\n",
    "            if hasattr(model, \"predict_proba\") and len(np.unique(y_te)) == 2:\n",
    "                try:\n",
    "                    prob = model.predict_proba(X_te_sel)[:, 1]\n",
    "                    roc = roc_auc_score(y_te, prob)\n",
    "                except Exception:\n",
    "                    prob = None\n",
    "                    roc = None\n",
    "\n",
    "            acc = accuracy_score(y_te, preds)\n",
    "            cls_report = classification_report(y_te, preds, zero_division=0)\n",
    "            cm = confusion_matrix(y_te, preds)\n",
    "\n",
    "            # Permutation importance (fast - few repeats)\n",
    "            try:\n",
    "                pi = permutation_importance(model, X_te_sel, y_te, n_repeats=3, random_state=RANDOM_STATE, n_jobs=1)\n",
    "                imp_df = pd.DataFrame({'feature': X_te_sel.columns, 'importance_mean': pi.importances_mean}).sort_values('importance_mean', ascending=False)\n",
    "            except Exception:\n",
    "                imp_df = pd.DataFrame({'feature': X_te_sel.columns, 'importance_mean': 0})\n",
    "\n",
    "            target_results[name] = {\n",
    "                'accuracy': acc,\n",
    "                'roc_auc': roc,\n",
    "                'classification_report': cls_report,\n",
    "                'confusion_matrix': cm.tolist(),\n",
    "                'permutation_importance': imp_df.head(15)\n",
    "            }\n",
    "\n",
    "            print(f\"[{target}] Model {name}: acc={acc:.3f}, roc={roc}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Model {name} failed for target {target}: {e}\")\n",
    "\n",
    "    results[target] = {\n",
    "        'selected_features': selected_cols,\n",
    "        'models': target_results,\n",
    "        'train_shape': X_tr_sel.shape,\n",
    "        'test_shape': X_te_sel.shape\n",
    "    }\n",
    "\n",
    "# --------------------------\n",
    "# 9. Save cleaned dataset and results\n",
    "# --------------------------\n",
    "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(Path(SAVE_DIR) / \"appendicitis_cleaned.csv\", index=False)\n",
    "pd.to_pickle(results, Path(SAVE_DIR) / \"modeling_results.pkl\")\n",
    "\n",
    "# Also save a CSV summary of model metrics\n",
    "rows = []\n",
    "for t, info in results.items():\n",
    "    for mname, mobj in info['models'].items():\n",
    "        rows.append({\n",
    "            \"target\": t,\n",
    "            \"model\": mname,\n",
    "            \"accuracy\": mobj.get('accuracy'),\n",
    "            \"roc_auc\": mobj.get('roc_auc')\n",
    "        })\n",
    "summary_df = pd.DataFrame(rows)\n",
    "summary_df.to_csv(Path(SAVE_DIR) / \"model_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved: appendicitis_cleaned.csv, modeling_results.pkl, model_summary.csv in\", SAVE_DIR)\n",
    "\n",
    "# --------------------------\n",
    "# 10. Quick descriptive analysis & plots (examples)\n",
    "# --------------------------\n",
    "# Descriptive table (first 20 rows)\n",
    "display(df.describe(include='all').T.head(20))\n",
    "\n",
    "# Correlations for numeric features\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numeric_features) >= 2:\n",
    "    corr = df[numeric_features].corr()\n",
    "    plt.figure(figsize=(8,6)); plt.title(\"Correlation matrix (numeric features)\")\n",
    "    plt.imshow(corr, aspect='auto'); plt.colorbar()\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.index)), corr.index)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Example histogram for top numeric columns (first 4)\n",
    "for c in numeric_features[:4]:\n",
    "    plt.figure(figsize=(5,3)); plt.title(f\"Histogram: {c}\")\n",
    "    plt.hist(df[c].dropna(), bins=30)\n",
    "    plt.xlabel(c); plt.ylabel(\"count\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# If desired: show top features for one target's best model\n",
    "for t, info in results.items():\n",
    "    print(\"\\nTarget:\", t)\n",
    "    for m, mv in info['models'].items():\n",
    "        print(\" Model:\", m, \" Accuracy:\", mv['accuracy'])\n",
    "        print(\" Top features by permutation importance:\")\n",
    "        display(mv['permutation_importance'].head(10))\n",
    "    break  # remove break if you want all targets\n",
    "\n",
    "# --------------------------\n",
    "# End of notebook\n",
    "# --------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
